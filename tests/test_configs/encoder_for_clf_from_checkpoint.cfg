[training]
batch_size=64
num_workers_for_dataloader=1

[training.trainer]
accelerator = "cpu"
strategy = "auto"
devices = "auto"
num_nodes= 1
precision = "32-true"
max_epochs = 1
min_epochs = null
max_steps = 10
min_steps = null
limit_train_batches = null
limit_val_batches = null
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0.0
val_check_interval = null
check_val_every_n_epoch = 1
num_sanity_val_steps = null
log_every_n_steps = 2
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = "logs/"

[training.trainer.logger]
@loggers = "wandb"
name = null
save_dir = "logs/"
version = null
offline = true
dir = null
id = null
anonymous = null
project = "snp-transformers"
checkpoint_name = null


[model]
@tasks = "classification_from_masked_lm"

[model.create_optimizer_fn]
@optimizers = "adam"
lr = 0.03


[model.encoder_for_masked_lm]
@tasks = "masked_lm_from_disk"
path = "tests/model_checkpoints/encoder_for_masked_lm.ckpt"


[dataset]

[dataset.training]
@datasets = "individuals_dataset"
path = "tests/data/data"

[dataset.validation]
@datasets = "individuals_dataset"
path = ${dataset.training.path}

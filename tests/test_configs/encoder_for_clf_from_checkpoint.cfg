[logger]

[logger.logger]
@loggers = "wandb"
name = null
save_dir = "logs/"
version = null
offline = true
dir = null
id = null
anonymous = null
project = "snp-transformers"
checkpoint_name = null

[train]
batch_size=64
num_workers_for_dataloader=1

[train.trainer]
accelerator = "cpu"
strategy = "auto"
devices = 1
num_nodes= 1
precision = "32-true"
max_epochs = 1
min_epochs = null
max_steps = 10
min_steps = null
limit_train_batches = null
limit_val_batches = null
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0.0
val_check_interval = null
check_val_every_n_epoch = 1
num_sanity_val_steps = null
log_every_n_steps = 2
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = "tests/model_checkpoints/tmp_logs"
logger = ${logger.logger}

[train.training_dataset]
@datasets = "individuals_dataset"
path = "tests/data/data"

[train.validation_dataset]
@datasets = "individuals_dataset"
path = ${train.training_dataset.path}


[model]
@tasks = "classification_from_masked_lm"

[model.create_optimizer_fn]
@optimizers = "adam"
lr = 0.03

[model.encoder_for_masked_lm]
@tasks = "masked_lm_from_disk"
path = "tests/model_checkpoints/encoder_for_masked_lm.ckpt"
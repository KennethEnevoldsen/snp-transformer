[logger]

[logger.logger]
@loggers = "wandb"
name = 100snps-fine-tune-no-pretrain-only-511-big-batch-5k-warmup
save_dir = logs/${logger.logger.name}
version = null
offline = false
dir = null
id = null
anonymous = null
project = "snp-transformers"
checkpoint_name = null

[train]
batch_size=256
num_workers_for_dataloader= 16

[train.trainer]
accelerator = "auto"
strategy = "auto"
# devices = "auto"
devices = "1,"
num_nodes= 1
precision="32-true"
# precision="bf16-mixed"
max_epochs = 10000
min_epochs = null
max_steps = 100000
min_steps = null
limit_train_batches = null
limit_val_batches = 4
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0
# val_check_interval = 200
val_check_interval = 0.25
check_val_every_n_epoch = 1
num_sanity_val_steps = null
log_every_n_steps = 1
# log_every_n_steps = 25
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 2
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = ${logger.logger.save_dir}
logger=${logger.logger}

[train.trainer.callbacks]
@callbacks = "callback_list"

[train.trainer.callbacks.*.early_stopping]
@callbacks = "early_stopping"
monitor = "Validation loss"
min_delta = 0.0
patience = 40

[train.trainer.callbacks.*.model_checkpoint]
@callbacks = "model_checkpoint"
monitor = "Validation loss"
mode = "min"
save_top_k = 1
save_dir = ${logger.logger.save_dir}

[train.trainer.callbacks.*.lr_monitor]
@callbacks = "learning_rate_monitor"


[train.training_dataset]
@datasets = "individuals_dataset"
path = ${train.validation_dataset.path}
split_path="/data-big-projects/snp-transformer/transfer/train.split"
pheno_dir=${train.validation_dataset.pheno_dir}
oversample_phenotypes=null
# oversample_alpha=1
oversample_alpha=0.5

[train.validation_dataset]
@datasets = "individuals_dataset"
path = "/data-big-projects/snp-transformer/100_snps/code511_mhc_eur"
split_path="/data-big-projects/snp-transformer/transfer/test.split"
pheno_dir="/data-big-projects/snp-transformer/transfer/phenos_only_511"
oversample_phenotypes=null
oversample_alpha=0.5

[apply]
batch_size=${train.batch_size}
num_workers_for_dataloader=${train.num_workers_for_dataloader}
output_path = logs/${logger.logger.name}/prediction.csv

[apply.trainer]
accelerator = ${train.trainer.accelerator}
strategy = ${train.trainer.strategy}
devices = ${train.trainer.devices}
num_nodes= 1
precision=${train.trainer.precision}
max_epochs = 10000
min_epochs = null
max_steps = 10000
min_steps = null
limit_train_batches = null
limit_val_batches = ${train.trainer.limit_val_batches}
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0
val_check_interval = ${train.trainer.val_check_interval}
check_val_every_n_epoch = ${train.trainer.check_val_every_n_epoch}
num_sanity_val_steps = ${train.trainer.num_sanity_val_steps}
log_every_n_steps = ${train.trainer.log_every_n_steps}
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = ${logger.logger.save_dir}
logger=${logger.logger}

[apply.dataset]
@datasets = "individuals_dataset"
path = ${train.validation_dataset.path}
pheno_dir = "/data-big-projects/snp-transformer/transfer/phenos_only_511"
# no need to split just apply it on the full dataset

[model]
@tasks = "classification"
phenotypes_to_predict = ["icd511"]

[model.create_optimizer_fn]
@optimizers = "adam"
# lr = 0.0006 <- why was it soo high? (from roberta) - with the batch size we should have it lower 
lr = 0.0001
# ^this is the max for bert in the paper

[model.create_scheduler_fn]
@lr_schedulers = "linear_schedule_with_warmup"
num_warmup_steps = 5000
num_training_steps = ${train.trainer.max_steps}
peak_lr = ${model.create_optimizer_fn.lr}



[model.embedding_module]
@embedders = "snp_embedder"
d_model = 768
dropout_prob = 0.1
max_sequence_length = 512
individuals = ${train.training_dataset}

[model.embedding_module.positional_embedding]
@embedders = "absolute_positional_embedding"
d_model = ${model.embedding_module.d_model}
dropout_prob = ${model.embedding_module.dropout_prob}
w_k_constant = 100000

[model.encoder_module]
@layers = "transformer_encoder"
num_layers = 12

[model.encoder_module.encoder_layer]
@layers = "transformer_encoder_layer"
d_model = ${model.embedding_module.d_model}
nhead = 12
dim_feedforward = 3072
layer_norm_eps = 1e-12
norm_first = true

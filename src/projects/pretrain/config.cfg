[training]
batch_size=64
num_workers_for_dataloader= 8

[training.trainer]
accelerator = "auto"
strategy = "auto"
devices = "auto"
num_nodes= 1
precision="32-true" 
max_epochs = 10
min_epochs = null
max_steps = -1
min_steps = null
limit_train_batches = null
limit_val_batches = 100
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0.0
val_check_interval = null
check_val_every_n_epoch = 1
num_sanity_val_steps = null
log_every_n_steps = 50
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = "logs/"

[training.trainer.logger]
@loggers = "wandb"
name = null
save_dir = "logs/"
version = null
offline = false
dir = null
id = null
anonymous = null
project = "snp-transformers"
checkpoint_name = null


[model]
@tasks = "masked_lm"
mask_phenotypes = true

[model.create_optimizer_fn]
@optimizers = "adam"
lr = 0.03

[model.embedding_module]
@embedders = "snp_embedder"
d_model = 32  
dropout_prob = 0.1
max_sequence_length = 128

[model.embedding_module.positional_embedding]
@embedders = "absolute_positional_embedding"
d_model = ${model.embedding_module.d_model}
dropout_prob = ${model.embedding_module.dropout_prob}
w_k_constant = 100000

[model.encoder_module]
@layers = "transformer_encoder"
num_layers = 2

[model.encoder_module.encoder_layer]
@layers = "transformer_encoder_layer"
d_model = ${model.embedding_module.d_model}
nhead = 8
dim_feedforward = 128
layer_norm_eps = 1e-12
norm_first = true

[dataset]

[dataset.training]
@datasets = "individuals_dataset"
path = "/data-big-projects/snp-transformer/transfer/mhc"

[dataset.validation]
@datasets = "individuals_dataset"
path = ${dataset.training.path}
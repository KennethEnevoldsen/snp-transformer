[logger]

[logger.logger]
@loggers = "wandb"
name = fine_tune_no_pretrain_multitask
save_dir = logs/${logger.logger.name}
version = null
offline = false
dir = null
id = null
anonymous = null
project = "snp-transformers"
checkpoint_name = null

[apply]
batch_size=4
num_workers_for_dataloader=16
output_path = logs/${logger.logger.name}/prediction.csv

[apply.trainer]
devices = "1,"
num_nodes= 1
max_epochs = 10000
min_epochs = null
max_steps = 10000
min_steps = null
limit_train_batches = null
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 1
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = ${logger.logger.save_dir}
logger=${logger.logger}

[apply.dataset]
@datasets = "individuals_dataset"
path = "/data-big-projects/snp-transformer/transfer/mhc_eur"
pheno_dir ="/data-big-projects/snp-transformer/transfer/phenos_41_499_511"
# no need to split just apply it on the full dataset

[model]
@tasks = "classification_from_disk"
phenotypes_to_predict = ["icd511", "icd41", "icd499"]
path = "/home/kenneth/github/snp-transformer/logs/fine_tune_no_pretrain_multitask/checkpoints/epoch=14-step=4710.ckpt"

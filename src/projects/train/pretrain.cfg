[logger]

[logger.logger]
@loggers = "wandb"
name = "pretrain"
save_dir = "logs/pretrain/"
version = null
offline = false
dir = null
id = null
anonymous = null
project = "snp-transformers"
checkpoint_name = null


[training]
batch_size=4
num_workers_for_dataloader= 16

[training.trainer]
accelerator = "auto"
strategy = "auto"
devices = "auto"
num_nodes= 1
precision="32-true"
# precision="bf16-mixed"
max_epochs = 100
min_epochs = null
max_steps = -1
min_steps = null
limit_train_batches = null
limit_val_batches = 100
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0.0
val_check_interval = null
check_val_every_n_epoch = 1
num_sanity_val_steps = null
log_every_n_steps = 10
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 16
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = "logs/pretrain/"
logger=${logger.logger}

[train.trainer.callbacks]
@callbacks = "callback_list"

[train.trainer.callbacks.*.early_stopping]
@callbacks = "early_stopping"
monitor = "Validation loss"
min_delta = 0.0
patience = 10

[train.trainer.callbacks.*.model_checkpoint]
@callbacks = "model_checkpoint"
monitor = "Validation loss"
mode = "min"
save_top_k = 1
save_dir = ${logger.logger.save_dir}

[train.trainer.callbacks.*.lr_monitor]
@callbacks = "learning_rate_monitor"


[train.training_dataset]
@datasets = "individuals_dataset"
path = "/data-big-projects/snp-transformer/transfer/mhc.eur"
split_path="/data-big-projects/snp-transformer/transfer/train.split"

[train.validation_dataset]
@datasets = "individuals_dataset"
path = "/data-big-projects/snp-transformer/transfer/mhc.eur"
split_path="/data-big-projects/snp-transformer/transfer/test.split"



[model]
@tasks = "masked_lm"
mask_phenotype = false

[model.create_optimizer_fn]
@optimizers = "adam"
lr = 0.0006

[model.embedding_module]
@embedders = "snp_embedder"
d_model = 768
dropout_prob = 0.1
max_sequence_length = 512
individuals = ${train.training_dataset}
checkpoint_path = "embedder"

[model.embedding_module.positional_embedding]
@embedders = "absolute_positional_embedding"
d_model = ${model.embedding_module.d_model}
dropout_prob = ${model.embedding_module.dropout_prob}
w_k_constant = 100000

[model.encoder_module]
@layers = "transformer_encoder"
num_layers = 12

[model.encoder_module.encoder_layer]
@layers = "transformer_encoder_layer"
d_model = ${model.embedding_module.d_model}
nhead = 12
dim_feedforward = 3072
layer_norm_eps = 1e-12
norm_first = true


[logger]

[logger.logger]
@loggers = "wandb"
name = "overfit-w-oversampling-fine-tune-no-pretrain-only-511"
save_dir = "logs/overfit-w-oversampling-fine-tune-no-pretrain-only-511"
# name = "fine-tune-no-pretrain-only-511"
# save_dir = "logs/fine-tune-no-pretrain-only-511"
version = null
offline = false
dir = null
id = null
anonymous = null
project = "snp-transformers"
checkpoint_name = null

[train]
batch_size=4
num_workers_for_dataloader= 16

[train.trainer]
accelerator = "auto"
strategy = "auto"
# devices = "auto"
devices = "1"
num_nodes= 1
precision="32-true"
# precision="bf16-mixed"
max_epochs = 10000
min_epochs = null
max_steps = -1
min_steps = null
limit_train_batches = null
# limit_val_batches = 100
limit_val_batches = 4
limit_test_batches = null
limit_predict_batches = null
overfit_batches = 0
val_check_interval = 200
# val_check_interval = 2000
check_val_every_n_epoch = null
num_sanity_val_steps = null
log_every_n_steps = 1
# log_every_n_steps = 25
enable_checkpointing = null
enable_progress_bar = null
enable_model_summary = null
accumulate_grad_batches = 16
gradient_clip_val = null
gradient_clip_algorithm = null
default_root_dir = ${logger.logger.save_dir}
logger=${logger.logger}

[train.trainer.callbacks]
@callbacks = "callback_list"

# [train.trainer.callbacks.*.early_stopping]
# @callbacks = "early_stopping"
# monitor = "Validation loss"
# min_delta = 0.0
# patience = 10

# [train.trainer.callbacks.*.model_checkpoint]
# @callbacks = "model_checkpoint"
# monitor = "Validation loss"
# mode = "min"
# save_top_k = 1
# save_dir = ${logger.logger.save_dir}

[train.trainer.callbacks.*.lr_monitor]
@callbacks = "learning_rate_monitor"


[train.training_dataset]
@datasets = "individuals_dataset"
path = "/data-big-projects/snp-transformer/transfer/mhc.eur"
split_path="/data-big-projects/snp-transformer/transfer/mini.split"
# split_path="/data-big-projects/snp-transformer/transfer/train.split"
pheno_dir=${train.validation_dataset.pheno_dir}
oversample_phenotypes=null
oversample_alpha=1
# oversample_alpha=0.5

[train.validation_dataset]
@datasets = "individuals_dataset"
path = "/data-big-projects/snp-transformer/transfer/mhc.eur"
split_path="/data-big-projects/snp-transformer/transfer/test.split"
pheno_dir="/data-big-projects/snp-transformer/transfer/phenos_only_511"
oversample_phenotypes=null
oversample_alpha=0.5

# [apply]
# trainer = ${train.trainer}
# batch_size=${train.batch_size}
# num_workers_for_dataloader=${train.num_workers_for_dataloader}
# output_path = "logs/fine-tune-no-pretrain/prediction_fine-tuned-no-pretrain.csv"

# [apply.dataset]
# @datasets = "individuals_dataset"
# path = "/data-big-projects/snp-transformer/transfer/mhc.eur"
# pheno_dir = "/data-big-projects/snp-transformer/transfer/phenos_only_511"
# # no need to split just apply it on the full dataset

[model]
@tasks = "classification"
phenotypes_to_predict = ["icd511"]

[model.create_optimizer_fn]
@optimizers = "adam"
# lr = 0.0006 <- why was it soo high?
lr = 0.000002
# ^this is the max for bert in the paper

[model.embedding_module]
@embedders = "snp_embedder"
d_model = 768
# dropout_prob = 0.1
dropout_prob = 0.01
max_sequence_length = 512
individuals = ${train.training_dataset}

[model.embedding_module.positional_embedding]
@embedders = "absolute_positional_embedding"
d_model = ${model.embedding_module.d_model}
dropout_prob = ${model.embedding_module.dropout_prob}
w_k_constant = 100000

[model.encoder_module]
@layers = "transformer_encoder"
num_layers = 12

[model.encoder_module.encoder_layer]
@layers = "transformer_encoder_layer"
d_model = ${model.embedding_module.d_model}
nhead = 12
dim_feedforward = 3072
layer_norm_eps = 1e-12
norm_first = true
